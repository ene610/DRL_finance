The maximize operator in formula (3) and (4) use the same Q-values to select and evaluate the greedy action.
 The consistency in action selection and action assessment makes it more prone to select the overestimated values as the es- timates of true action values.
 To prevent this, Double Q- learning[3] decouples the action selection from action evalu- ation.
 The Double Q-learning algorithm has two Q-networks separately with weights θA and θB.
 To obtain the update target at each step, it randomly choose one set of weights to determine the greedy policy in the given state st+1 and let the remaining set of weights to determine its corresponding action value.
 Accordingly, the target of Double Q-learning to update Q-network A can be written as follows yQA t = rt+1 + γQ(st+1, arg maxa′Q(st+1, a ′; θA); θB).
 (5) Formula (5) shows that we are still using the current online Q-network A to choose the greedy policies, and then us- ing Q-network B to estimate the value of the corresponding greedy policies.
 Conversely, if we want to update the param- eters of Q-network B, then swap the roles of Q-network A and Q-network B.
 So the update targets for the second set of weights θB can be written as following yQB t = rt+1 + γQ(st+1, arg maxa′Q(st+1, a ′; θB); θA).
 (6) Double Q-learning stores two Q functions, and each Q func- tion is updated with Q-values from the remaining Q function for the next state.
 